{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\n",
      "/kaggle/input/glove-twitter-27b-200d-txt/glove.twitter.27B.200d.txt\n",
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv\n",
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\n",
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv\n",
      "/kaggle/input/toxic-data-preprocessing/__output__.json\n",
      "/kaggle/input/toxic-data-preprocessing/test_preprocessed.csv\n",
      "/kaggle/input/toxic-data-preprocessing/train_preprocessed.csv\n"
     ]
    }
   ],
   "source": [
    "# Toxic Comments - preprocess+embed+gru+conv1d\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import gc\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from unidecode import unidecode\n",
    "import time\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "data_paths = {}\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        data_paths[filename] = os.path.join(dirname, filename)\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (159571, 8)\n",
      "Columns in Train: Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
      "       'insult', 'identity_hate'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(data_paths['train.csv'])\n",
    "test_df = pd.read_csv(data_paths['test.csv'])\n",
    "sub_df = pd.read_csv(data_paths['sample_submission.csv'])\n",
    "print('Train shape:', train_df.shape)\n",
    "print('Columns in Train:', train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_col = ['id', 'is_clean']\n",
    "text_col = ['comment_text']\n",
    "num_col = ['total_len', 'sent_count','word_count', 'capitals', 'punct_count', 'smilies_count',\n",
    "           'unique_word_count', 'unique_word_percent']\n",
    "label_col = [col for col in train_df.columns if col not in text_col + drop_col]\n",
    "label_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    \n",
    "    df['total_length'] = df['comment_text'].apply(len)\n",
    "    df['capitals'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                axis=1)\n",
    "    df['num_words'] = df.comment_text.str.count('\\S+')\n",
    "    df['num_unique_words'] = df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']  \n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = add_features(train_df)\n",
    "test_df = add_features(test_df)\n",
    "\n",
    "train_df = train_df.fillna(0)\n",
    "test_df = test_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "special_character_removal = re.compile(r'[^A-Za-z\\.\\-\\?\\!\\,\\#\\@\\% ]',re.IGNORECASE)\n",
    "def clean_text(x):\n",
    "    x_ascii = unidecode(x)\n",
    "    x_clean = special_character_removal.sub('',x_ascii)\n",
    "    return x_clean\n",
    "\n",
    "train_df['clean_text'] = pd.read_csv(data_paths['train_preprocessed.csv'])['comment_text']\n",
    "test_df['clean_text'] = pd.read_csv(data_paths['test_preprocessed.csv'])['comment_text']\n",
    "\n",
    "train_df = train_df.fillna(' ')\n",
    "test_df = test_df.fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 2) (153164, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "train_val_counts = train_df[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
    "test_counts = test_df[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
    "\n",
    "ss.fit(train_val_counts)\n",
    "train_val_counts = ss.transform(train_val_counts)\n",
    "test_counts = ss.transform(test_counts)\n",
    "\n",
    "print(train_val_counts.shape, test_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 250) (159571, 6) (153164, 250)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy\n",
    "\n",
    "X_train_val = train_df[text_col].values.ravel()\n",
    "y_train_val = train_df[label_col].values\n",
    "X_test = test_df[text_col].values.ravel()\n",
    "\n",
    "\n",
    "# intialize param\n",
    "max_features = 90000\n",
    "maxlen = 250\n",
    "\n",
    "# build vocab\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train_val)\n",
    "\n",
    "# sequences\n",
    "X_train_val = tokenizer.texts_to_sequences(X_train_val)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# padded sequences\n",
    "X_train_val = sequence.pad_sequences(X_train_val, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train_val.shape, y_train_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed loading fasttext vector file\n",
      "completed loading fasttext embeddings\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "\n",
    "# FASTTEXT\n",
    "\n",
    "FASTTEXT_FILE = data_paths['crawl-300d-2M.vec']\n",
    "fasttext_size = 300\n",
    "\n",
    "EMBEDDING_FILE = open(FASTTEXT_FILE)\n",
    "\n",
    "fasttext_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in EMBEDDING_FILE)\n",
    "EMBEDDING_FILE.close()\n",
    "print('completed loading fasttext vector file')\n",
    "\n",
    "# intialize embedding matrix\n",
    "fasttext_matrix = np.zeros((nb_words, fasttext_size))\n",
    "# \n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = fasttext_index.get(word)\n",
    "    if embedding_vector is not None: fasttext_matrix[i] = embedding_vector\n",
    "        \n",
    "print('completed loading fasttext embeddings')\n",
    "        \n",
    "del fasttext_index\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- references:\n",
    "    - https://www.kaggle.com/fizzbuzz/toxic-data-preprocessing\n",
    "    - https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52581#latest-302637\n",
    "    - https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52647#latest-300704\n",
    "- trained embeddings + GRU(16) = 0.9753, 0.9752\n",
    "- pretrained fasttext embeddings + GRU(128) + Conv1d(64) = 0.9843, 0.9840\n",
    "- improved preprocessing + pretrained fasttext embeddings + GRU(128) + Conv1d(64) = 0.9849, 0.9851"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras import optimizers, callbacks, regularizers\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate, Conv1D, Layer\n",
    "from keras.layers import GRU,LSTM, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import CuDNNGRU, CuDNNLSTM\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        self.max_score = 0\n",
    "        self.not_better_count = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=1)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "            if (score > self.max_score):\n",
    "                print(\"### New High Score (previous: %.6f) ###\\n\" % self.max_score)\n",
    "                model.save_weights(\"best_weights.h5\")\n",
    "                self.max_score=score\n",
    "                self.not_better_count = 0\n",
    "            else:\n",
    "                self.not_better_count += 1\n",
    "                if self.not_better_count > 3:\n",
    "                    print(\"Epoch %05d: early stopping, high score = %.6f\" % (epoch,self.max_score))\n",
    "                    self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(train_counts):\n",
    "        inp = Input(shape=(maxlen, ))\n",
    "        count_input = Input(shape=(train_counts.shape[1],))\n",
    "        x = Embedding(max_features, fasttext_size, weights=[fasttext_matrix], trainable=False)(inp)\n",
    "        x = SpatialDropout1D(0.5)(x)\n",
    "        x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x) #80\n",
    "        x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "        avg_pool = GlobalAveragePooling1D()(x)\n",
    "        max_pool = GlobalMaxPooling1D()(x)\n",
    "        conc = concatenate([avg_pool, max_pool, count_input])\n",
    "        out = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "        model = Model(inputs=[inp,count_input], outputs=out)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=optimizers.Adam(lr=0.001),\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "# model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "143613/143613 [==============================] - 96s 670us/step - loss: 0.0540 - accuracy: 0.9805\n",
      "15958/15958 [==============================] - 4s 256us/step\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.980418 \n",
      "\n",
      "### New High Score (previous: 0.000000) ###\n",
      "\n",
      "Epoch 2/10\n",
      "143613/143613 [==============================] - 95s 658us/step - loss: 0.0452 - accuracy: 0.9829\n",
      "15958/15958 [==============================] - 4s 251us/step\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.983300 \n",
      "\n",
      "### New High Score (previous: 0.980418) ###\n",
      "\n",
      "Epoch 3/10\n",
      "143613/143613 [==============================] - 94s 657us/step - loss: 0.0421 - accuracy: 0.9839\n",
      "15958/15958 [==============================] - 4s 256us/step\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.985729 \n",
      "\n",
      "### New High Score (previous: 0.983300) ###\n",
      "\n",
      "Epoch 4/10\n",
      "143613/143613 [==============================] - 94s 658us/step - loss: 0.0393 - accuracy: 0.9848\n",
      "15958/15958 [==============================] - 4s 249us/step\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.984280 \n",
      "\n",
      "Epoch 6/10\n",
      "143613/143613 [==============================] - 94s 657us/step - loss: 0.0382 - accuracy: 0.9851\n",
      "15958/15958 [==============================] - 4s 250us/step\n",
      "\n",
      " ROC-AUC - epoch: 6 - score: 0.986346 \n",
      "\n",
      "### New High Score (previous: 0.985729) ###\n",
      "\n",
      "Epoch 7/10\n",
      "143613/143613 [==============================] - 95s 661us/step - loss: 0.0374 - accuracy: 0.9854\n",
      "15958/15958 [==============================] - 4s 249us/step\n",
      "\n",
      " ROC-AUC - epoch: 7 - score: 0.985107 \n",
      "\n",
      "Epoch 8/10\n",
      "143613/143613 [==============================] - 95s 658us/step - loss: 0.0361 - accuracy: 0.9859\n",
      "15958/15958 [==============================] - 4s 254us/step\n",
      "\n",
      " ROC-AUC - epoch: 8 - score: 0.982835 \n",
      "\n",
      "Epoch 9/10\n",
      "15957/15957 [==============================] - 4s 248us/step\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.990295 \n",
      "\n",
      "### New High Score (previous: 0.989509) ###\n",
      "\n",
      "Epoch 4/10\n",
      "143614/143614 [==============================] - 95s 663us/step - loss: 0.0413 - accuracy: 0.9841\n",
      "15957/15957 [==============================] - 4s 251us/step\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.989892 \n",
      "\n",
      "Epoch 5/10\n",
      "143614/143614 [==============================] - 96s 672us/step - loss: 0.0427 - accuracy: 0.9836\n",
      "15957/15957 [==============================] - 4s 258us/step\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.985643 \n",
      "\n",
      "Epoch 4/10\n",
      "143614/143614 [==============================] - 97s 675us/step - loss: 0.0408 - accuracy: 0.9841\n",
      "15957/15957 [==============================] - 4s 252us/step\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.987211 \n",
      "\n",
      "### New High Score (previous: 0.986210) ###\n",
      "\n",
      "Epoch 5/10\n",
      " 73088/143614 [==============>...............] - ETA: 46s - loss: 0.0377 - accuracy: 0.9853"
     ]
    }
   ],
   "source": [
    "num_folds = 10 # folds\n",
    "\n",
    "y_test_predict = np.zeros((test_df.shape[0],6))\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=1, factor = 0.5, monitor='val_loss', verbose=1)\n",
    "\n",
    "kf = KFold(n_splits = num_folds, shuffle = True, random_state = 2019)\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_val):\n",
    "\n",
    "    kfold_y_train, kfold_y_val = y_train_val[train_index], y_train_val[val_index]\n",
    "    kfold_X_train, kfold_X_train_counts = X_train_val[train_index], train_val_counts[train_index]\n",
    "    kfold_X_valid, kfold_X_val_counts = X_train_val[val_index], train_val_counts[val_index]\n",
    "    \n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    \n",
    "    model = get_model(kfold_X_train_counts)\n",
    "    \n",
    "    ra_val = RocAucEvaluation(validation_data=([kfold_X_valid,kfold_X_val_counts], kfold_y_val), interval = 1)\n",
    "    \n",
    "    model.fit([kfold_X_train,kfold_X_train_counts], kfold_y_train, \n",
    "              batch_size = 32, epochs = 10, verbose=1, callbacks = [ra_val, reduce_lr])\n",
    "    gc.collect()\n",
    "    \n",
    "    model.load_weights(\"best_weights.h5\")\n",
    "    \n",
    "    y_test_predict += model.predict([X_test,test_counts], batch_size = 256, verbose=1) / num_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.996620</td>\n",
       "      <td>0.400527</td>\n",
       "      <td>0.987518</td>\n",
       "      <td>0.155343</td>\n",
       "      <td>0.967755</td>\n",
       "      <td>0.534644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.001511</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.005477</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0  00001cee341fdb12  0.996620      0.400527  0.987518  0.155343  0.967755   \n",
       "1  0000247867823ef7  0.001511      0.000031  0.000238  0.000016  0.000252   \n",
       "2  00013b17ad220c46  0.000760      0.000061  0.000221  0.000027  0.000273   \n",
       "3  00017563c3f7919a  0.000261      0.000012  0.000119  0.000022  0.000188   \n",
       "4  00017695ad8997eb  0.005477      0.000129  0.000899  0.000119  0.000673   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.534644  \n",
       "1       0.000039  \n",
       "2       0.000062  \n",
       "3       0.000010  \n",
       "4       0.000074  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='bi-gru-conv1d-kfold-preprocessed.csv' target='_blank'>bi-gru-conv1d-kfold-preprocessed.csv</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/bi-gru-conv1d-kfold-preprocessed.csv"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df.iloc[:,1:] = y_test_predict\n",
    "display(sub_df.head())\n",
    "from IPython.display import FileLink\n",
    "sub_name = 'bi-gru-conv1d-kfold-preprocessed.csv'\n",
    "sub_df.to_csv(sub_name, index = None)\n",
    "FileLink(sub_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# # plot training & validation results\n",
    "# df = pd.DataFrame()\n",
    "# df['train_loss'] = hist.history['loss']\n",
    "# df['val_loss'] = hist.history['val_loss']\n",
    "# df.index = np.arange(1,len(df)+1,1)\n",
    "    \n",
    "# # draw Loss\n",
    "# df[['train_loss', 'val_loss']].plot()\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import sys\n",
    "# from scipy.stats import ks_2samp\n",
    "\n",
    "# first_file = sys.argv[1]\n",
    "# second_file = sys.argv[2]\n",
    "\n",
    "# def corr(first_file, second_file):\n",
    "#     # assuming first column is `class_name_id`\n",
    "#     first_df = pd.read_csv(first_file, index_col=0)\n",
    "#     second_df = pd.read_csv(second_file, index_col=0)\n",
    "#     class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "#     for class_name in class_names:\n",
    "#         # all correlations\n",
    "#         print('\\n Class: %s' % class_name)\n",
    "#         print(' Pearson\\'s correlation score: %0.6f' %\n",
    "#               first_df[class_name].corr(\n",
    "#                   second_df[class_name], method='pearson'))\n",
    "#         print(' Kendall\\'s correlation score: %0.6f' %\n",
    "#               first_df[class_name].corr(\n",
    "#                   second_df[class_name], method='kendall'))\n",
    "#         print(' Spearman\\'s correlation score: %0.6f' %\n",
    "#               first_df[class_name].corr(\n",
    "#                   second_df[class_name], method='spearman'))\n",
    "#         ks_stat, p_value = ks_2samp(first_df[class_name].values,\n",
    "#                                     second_df[class_name].values)\n",
    "#         print(' Kolmogorov-Smirnov test:    KS-stat = %.6f    p-value = %.3e\\n'\n",
    "#               % (ks_stat, p_value))\n",
    "\n",
    "# corr(first_file, second_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
