{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\n",
      "/kaggle/input/glove-twitter-27b-200d-txt/glove.twitter.27B.200d.txt\n",
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv\n",
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\n",
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv\n"
     ]
    }
   ],
   "source": [
    "# Toxic Comments - preprocess+embed+Lstm+Gru\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import gc\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from unidecode import unidecode\n",
    "import time\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "data_paths = {}\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        data_paths[filename] = os.path.join(dirname, filename)\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (159571, 8)\n",
      "Columns in Train: Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
      "       'insult', 'identity_hate'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(data_paths['train.csv'])\n",
    "test_df = pd.read_csv(data_paths['test.csv'])\n",
    "sub_df = pd.read_csv(data_paths['sample_submission.csv'])\n",
    "print('Train shape:', train_df.shape)\n",
    "print('Columns in Train:', train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_col = ['id', 'is_clean']\n",
    "text_col = ['comment_text']\n",
    "num_col = ['total_len', 'sent_count','word_count', 'capitals', 'punct_count', 'smilies_count',\n",
    "           'unique_word_count', 'unique_word_percent']\n",
    "label_col = [col for col in train_df.columns if col not in text_col + drop_col + num_col]\n",
    "label_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "special_character_removal = re.compile(r'[^A-Za-z\\.\\-\\?\\!\\,\\#\\@\\% ]',re.IGNORECASE)\n",
    "def clean_text(x):\n",
    "    x_ascii = unidecode(x)\n",
    "    x_clean = special_character_removal.sub('',x_ascii)\n",
    "    return x_clean\n",
    "\n",
    "train_df['clean_text'] = train_df['comment_text'].apply(lambda x: clean_text(str(x)))\n",
    "test_df['clean_text'] = test_df['comment_text'].apply(lambda x: clean_text(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.fillna(' ')\n",
    "test_df = test_df.fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    \n",
    "    df['total_length'] = df['comment_text'].apply(len)\n",
    "    df['capitals'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                axis=1)\n",
    "    df['num_words'] = df.comment_text.str.count('\\S+')\n",
    "    df['num_unique_words'] = df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']  \n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = add_features(train_df)\n",
    "test_df = add_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 2) (153164, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "train_val_counts = train_df[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
    "test_counts = test_df[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
    "\n",
    "ss.fit(train_val_counts)\n",
    "train_val_counts = ss.transform(train_val_counts)\n",
    "test_counts = ss.transform(test_counts)\n",
    "\n",
    "print(train_val_counts.shape, test_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 520) (159571, 6) (153164, 520)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy\n",
    "\n",
    "X_train_val = train_df[text_col].values.ravel()\n",
    "y_train_val = train_df[label_col].values\n",
    "X_test = test_df[text_col].values.ravel()\n",
    "\n",
    "# intialize param\n",
    "max_features = 200000\n",
    "maxlen = 520\n",
    "\n",
    "# build vocab\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train_val)\n",
    "\n",
    "# sequences\n",
    "X_train_val = tokenizer.texts_to_sequences(X_train_val)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# padded sequences\n",
    "X_train_val = sequence.pad_sequences(X_train_val, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train_val.shape, y_train_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed loading fasttext vector file\n",
      "completed loading fasttext embeddings\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "\n",
    "# FASTTEXT\n",
    "\n",
    "FASTTEXT_FILE = data_paths['crawl-300d-2M.vec']\n",
    "fasttext_size = 300\n",
    "\n",
    "EMBEDDING_FILE = open(FASTTEXT_FILE)\n",
    "\n",
    "fasttext_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in EMBEDDING_FILE)\n",
    "EMBEDDING_FILE.close()\n",
    "print('completed loading fasttext vector file')\n",
    "\n",
    "# intialize embedding matrix\n",
    "fasttext_matrix = np.zeros((nb_words, fasttext_size))\n",
    "# \n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = fasttext_index.get(word)\n",
    "    if embedding_vector is not None: fasttext_matrix[i] = embedding_vector\n",
    "        \n",
    "print('completed loading fasttext embeddings')\n",
    "        \n",
    "del fasttext_index\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed loading glove vector file\n",
      "completed loading glove embeddings\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GLOVE_FILE = data_paths['glove.twitter.27B.200d.txt']\n",
    "glove_size = 200\n",
    "\n",
    "EMBEDDING_FILE = open(GLOVE_FILE)\n",
    "\n",
    "glove_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in EMBEDDING_FILE)\n",
    "EMBEDDING_FILE.close()\n",
    "print('completed loading glove vector file')\n",
    "\n",
    "# intialize embedding matrix\n",
    "glove_matrix = np.zeros((nb_words, glove_size))\n",
    "# \n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = glove_index.get(word)\n",
    "    if embedding_vector is not None: glove_matrix[i] = embedding_vector\n",
    "        \n",
    "print('completed loading glove embeddings')\n",
    "\n",
    "del glove_index\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_size = fasttext_size + glove_size # i.e. 300+200\n",
    "embedding_matrix = np.concatenate((fasttext_matrix, glove_matrix), axis = 1)\n",
    "del fasttext_matrix, glove_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- references:\n",
    "    - https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644#latest-319962\n",
    "    - https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52564#latest-610859"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras import optimizers, callbacks, regularizers\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate, Conv1D, Layer\n",
    "from keras.layers import GRU,LSTM, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import CuDNNGRU, CuDNNLSTM\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        self.max_score = 0\n",
    "        self.not_better_count = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=1)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "            if (score > self.max_score):\n",
    "                print(\"*** New High Score (previous: %.6f) \\n\" % self.max_score)\n",
    "                model.save_weights(\"best_weights.h5\")\n",
    "                self.max_score=score\n",
    "                self.not_better_count = 0\n",
    "            else:\n",
    "                self.not_better_count += 1\n",
    "                if self.not_better_count > 3:\n",
    "                    print(\"Epoch %05d: early stopping, high score = %.6f\" % (epoch,self.max_score))\n",
    "                    self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(train_counts): # fasttext_size, fasttext_matrix\n",
    "        count_input = Input(shape=(train_counts.shape[1],))\n",
    "        inp = Input(shape=(maxlen,))\n",
    "        x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "        x = SpatialDropout1D(0.5)(x)\n",
    "        x = Bidirectional(CuDNNLSTM(40, return_sequences=True))(x)\n",
    "        x, x_h, x_c = Bidirectional(CuDNNGRU(40, return_sequences=True, return_state = True))(x)  \n",
    "        avg_pool = GlobalAveragePooling1D()(x)\n",
    "        max_pool = GlobalMaxPooling1D()(x)\n",
    "        conc = concatenate([avg_pool, x_h, max_pool,count_input])\n",
    "        out = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "        model = Model(inputs=[inp,count_input], outputs=out)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=optimizers.Adam(lr=0.001, clipvalue = 1.0),\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "# model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "143613/143613 [==============================] - 224s 2ms/step - loss: 0.0560 - accuracy: 0.9800\n",
      "15958/15958 [==============================] - 18s 1ms/step\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.979051 \n",
      "\n",
      "*** New High Score (previous: 0.000000) \n",
      "\n",
      "Epoch 2/10\n",
      "143613/143613 [==============================] - 222s 2ms/step - loss: 0.0419 - accuracy: 0.9836\n",
      "15958/15958 [==============================] - 17s 1ms/step\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.983700 \n",
      "\n",
      "*** New High Score (previous: 0.979051) \n",
      "\n",
      "Epoch 3/10\n",
      "143613/143613 [==============================] - 223s 2ms/step - loss: 0.0380 - accuracy: 0.9849\n",
      "15958/15958 [==============================] - 18s 1ms/step\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.987255 \n",
      "\n",
      "*** New High Score (previous: 0.985944) \n",
      "\n",
      "Epoch 5/10\n",
      "143613/143613 [==============================] - 222s 2ms/step - loss: 0.0368 - accuracy: 0.9852\n",
      "15958/15958 [==============================] - 17s 1ms/step\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.987226 \n",
      "\n",
      "Epoch 6/10\n",
      "143613/143613 [==============================] - 222s 2ms/step - loss: 0.0357 - accuracy: 0.9856\n",
      "15958/15958 [==============================] - 17s 1ms/step\n",
      "\n",
      " ROC-AUC - epoch: 6 - score: 0.988158 \n",
      "\n",
      "*** New High Score (previous: 0.987255) \n",
      "\n",
      "Epoch 7/10\n",
      "143614/143614 [==============================] - 218s 2ms/step - loss: 0.0335 - accuracy: 0.9864\n",
      "15957/15957 [==============================] - 17s 1ms/step\n",
      "\n",
      " ROC-AUC - epoch: 9 - score: 0.988256 \n",
      "\n",
      "Epoch 10/10\n",
      "143614/143614 [==============================] - 220s 2ms/step - loss: 0.0351 - accuracy: 0.9859\n",
      "143614/143614 [==============================] - 219s 2ms/step - loss: 0.0337 - accuracy: 0.9863\n",
      "15957/15957 [==============================] - 17s 1ms/step\n",
      "\n",
      " ROC-AUC - epoch: 9 - score: 0.991497 \n",
      "\n",
      "Epoch 10/10\n",
      "143614/143614 [==============================] - 220s 2ms/step - loss: 0.0566 - accuracy: 0.9800\n",
      "15957/15957 [==============================] - 17s 1ms/step\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.991490 \n",
      "\n",
      "*** New High Score (previous: 0.990491) \n",
      "\n",
      "Epoch 4/10\n",
      "143614/143614 [==============================] - 218s 2ms/step - loss: 0.0360 - accuracy: 0.9856\n",
      "15957/15957 [==============================] - 17s 1ms/step\n",
      "\n",
      " ROC-AUC - epoch: 6 - score: 0.991680 \n",
      "\n",
      "Epoch 7/10\n",
      "143614/143614 [==============================] - 217s 2ms/step - loss: 0.0385 - accuracy: 0.9847\n",
      "15957/15957 [==============================] - 16s 1ms/step\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.991665 \n",
      "\n",
      "*** New High Score (previous: 0.991583) \n",
      "\n",
      "Epoch 5/10\n",
      "143614/143614 [==============================] - 219s 2ms/step - loss: 0.0369 - accuracy: 0.9852\n",
      "15957/15957 [==============================] - 17s 1ms/step\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.990330 \n",
      "\n",
      "*** New High Score (previous: 0.989814) \n",
      "\n",
      "Epoch 6/10\n",
      "143614/143614 [==============================] - 219s 2ms/step - loss: 0.0424 - accuracy: 0.9836\n",
      "15957/15957 [==============================] - 17s 1ms/step\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.988961 \n",
      "\n",
      "*** New High Score (previous: 0.984419) \n",
      "\n",
      "Epoch 3/10\n",
      "143614/143614 [==============================] - 219s 2ms/step - loss: 0.0327 - accuracy: 0.9866\n",
      " 5920/15957 [==========>...................] - ETA: 10s"
     ]
    }
   ],
   "source": [
    "num_folds = 10 # folds\n",
    "\n",
    "y_test_predict = np.zeros((test_df.shape[0],6))\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(patience=3, monitor='val_loss', verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=1, factor=0.5, monitor='val_loss', verbose=1)\n",
    "\n",
    "kf = KFold(n_splits = num_folds, shuffle = True, random_state = 2019)\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_val):\n",
    "\n",
    "    kfold_y_train, kfold_y_val = y_train_val[train_index], y_train_val[val_index]\n",
    "    kfold_X_train, kfold_X_train_counts = X_train_val[train_index], train_val_counts[train_index]\n",
    "    kfold_X_valid, kfold_X_val_counts = X_train_val[val_index], train_val_counts[val_index]\n",
    "    \n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    \n",
    "    model = get_model(kfold_X_train_counts)\n",
    "    \n",
    "    ra_val = RocAucEvaluation(validation_data=([kfold_X_valid,kfold_X_val_counts], kfold_y_val), interval = 1)\n",
    "    \n",
    "    model.fit([kfold_X_train,kfold_X_train_counts], kfold_y_train, \n",
    "              batch_size = 64, epochs = 10, verbose=1, callbacks = [ra_val, early_stop, reduce_lr])\n",
    "    gc.collect()\n",
    "    \n",
    "    model.load_weights(\"best_weights.h5\")\n",
    "    \n",
    "    y_test_predict += model.predict([X_test,test_counts], batch_size = 256, verbose=1) / num_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.996907</td>\n",
       "      <td>0.461211</td>\n",
       "      <td>0.981336</td>\n",
       "      <td>0.142995</td>\n",
       "      <td>0.955312</td>\n",
       "      <td>0.485726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0  00001cee341fdb12  0.996907      0.461211  0.981336  0.142995  0.955312   \n",
       "1  0000247867823ef7  0.000383      0.000007  0.000046  0.000003  0.000038   \n",
       "2  00013b17ad220c46  0.000113      0.000008  0.000042  0.000002  0.000030   \n",
       "3  00017563c3f7919a  0.000082      0.000004  0.000018  0.000008  0.000018   \n",
       "4  00017695ad8997eb  0.001362      0.000034  0.000216  0.000038  0.000130   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.485726  \n",
       "1       0.000007  \n",
       "2       0.000009  \n",
       "3       0.000003  \n",
       "4       0.000021  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='bi-lstm-gru-kfold-sub.csv' target='_blank'>bi-lstm-gru-kfold-sub.csv</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/bi-lstm-gru-kfold-sub.csv"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df.iloc[:,1:] = y_test_predict\n",
    "display(sub_df.head())\n",
    "from IPython.display import FileLink\n",
    "sub_name = 'bi-lstm-gru-kfold-sub.csv'\n",
    "sub_df.to_csv(sub_name, index = None)\n",
    "FileLink(sub_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# # plot training & validation results\n",
    "# df = pd.DataFrame()\n",
    "# df['train_loss'] = hist.history['loss']\n",
    "# df['val_loss'] = hist.history['val_loss']\n",
    "# df.index = np.arange(1,len(df)+1,1)\n",
    "    \n",
    "# # draw Loss\n",
    "# df[['train_loss', 'val_loss']].plot()\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
