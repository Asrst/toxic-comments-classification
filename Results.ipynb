{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning to Identify Toxic Comments\n",
    "\n",
    "### Overview:\n",
    "\n",
    "More details of the competition can be found here: \n",
    "> source: [kaggle](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)\n",
    "\n",
    "### Results:\n",
    "\n",
    "#### 1. 01-Eda-baselines.ipynb\n",
    "\n",
    "\n",
    "- Linear models are very well suited for this problem\n",
    "- Logistic Regression is having better baseline score than any other model\n",
    "- Logistic also has an edge with lower time/space complexity compared to ensembles and also, we can impove this further by text preprocessing & hyper parameter tuning.\n",
    "- So rather than trying complex models, I will settle with linear models & will try to improve the performance using preprocessing, finetuning & feature engineering. My next target is to improve to score to more than 0.98 using only linear models.\n",
    "\n",
    " **Baseline-Scores**\n",
    "\n",
    "| Model         | Public LB | Private LB | Comments |\n",
    "| ------------- | --------- | ---------- | -------- |\n",
    "| Naive Bayes   | 0.89144   | 0.88520    | tfidf    |\n",
    "| Logistic      | 0.96877   | 0.96534    | tfidf    |\n",
    "| Linear SVM    | 0.95035   | 0.94956    | tfidf    |\n",
    "| Random Forest | 0.91497   | 0.93083    | tfidf    |\n",
    "| XGB/LGB       | 0.91114   | 0.93331    | tfidf    |\n",
    "\n",
    "\n",
    "\n",
    "#### 02-Improving-linear-model-baselines.ipynb\n",
    "\n",
    "- Finally, after lots of tries, I achevied the 0.981 using the NBLogistic (Naive Bayes + Logistic) Model on one run. With K-fold cross validation that score improved a bit to 0.9816.\n",
    "- In case of linear model, minimal proprocessing of text data (lowering, punctuations & stopwords removal) gave the better results. I tried few other pre-processings like lemmatization/emoji-conversion but didn't get good results.\n",
    "- I experimented with xgb & lgbm if they can beat the above score, but they were not anywhere near, best being only around 0.96(xgb).\n",
    "\n",
    " **Improved linear models**\n",
    " \n",
    " - tfidf_word & tfidf_char features were concatenated.\n",
    " - regualarisation parameter is tuned.\n",
    " - performed k-fold (5-folds) cross validation on NBLogisitc:\n",
    "\n",
    "\n",
    "| Model            | Public LB | Private LB | Comments              |\n",
    "| -----------------| --------- | ---------- | ----------------------|\n",
    "| Naive Bayes      | 0.90931   | 0.90696    | tfidf - (words + char)|\n",
    "| Logistic         | 0.97550   | 0.97394    | tfidf - (words + char)|\n",
    "| Linear SVM       | 0.96350   | 0.96956    | tfidf - (words + char)|\n",
    "| NBLogistic       | 0.97819   | 0.97664    | tfidf - (words + char)|\n",
    "| NBLogistic(5fold)| 0.98160   | 0.98201    | tfidf - (words + char)|\n",
    "\n",
    "\n",
    "#### 03-Embed + Gru+ Conv1d.ipynb\n",
    "\n",
    "- The highest score in kaggle is 0.989...I just want know what's the effort needed to improve the score to > 0.985.Here are my observations\n",
    "- Without use of pretrained embedding the scores for deep learning models are also similar to that of linear models lying around 0.98.\n",
    "- In case of deep neural net models, proprocessing of text data seem to have affect especially to cross that 0.985 barrier. I took references from this [zafar's script](https://www.kaggle.com/fizzbuzz/toxic-data-preprocessing) for text pre-processing and it helped to improve my score by 0.1.\n",
    "- Experimented with glove & fasttext (300-dimiensions) embedding...especially preprocessing + fasttext embeddings helped me to take the score of the 0.9845, while the \n",
    "model trained with glove scored around 0.983\n",
    "- This above notebook contains the architecture on which I obtained the 0.9851 on the public leaderboard but it was down to 0.9848 on private leaderbaord.\n",
    "\n",
    " **deep neural nets**\n",
    "\n",
    "| Model                                | Public LB | Private LB | Comments |\n",
    "| -------------------------------------| --------- | ---------- | -------- |\n",
    "| embed + gru                          | 0.98104   | 0.97952    |          |\n",
    "| fasttext embed + gru                 | 0.98334   | 0.97305    |          |\n",
    "| embed+gru+conv1d (minimal preprocess)| 0.98395   | 0.98353    |          |\n",
    "| embed+gru+conv1d (regex preprocess)  | 0.98433   | 0.98402    |          |\n",
    "| fastext embed + gru + conv1d (regex) | 0.98516   | 0.98486    |          |\n",
    "\n",
    "#### 04-Embed + BiLstm.ipynb\n",
    "\n",
    "- With architecture in the above notebook I crossed my target of 0.9850 on both on the public and private leaderbaords.\n",
    "- blending & stacking predictions of different models whose correlation is low. And these are few references I followed to do the same\n",
    "    - [mlwave](https://mlwave.com/kaggle-ensembling-guide/)\n",
    "    - [kaggle blog](http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/)\n",
    "\n",
    "| Model                                     | Public LB | Private LB | Comments |\n",
    "| ------------------------------------------| --------- | ---------- | -------- |\n",
    "| fasttext embed+Lstm                       | 0.98390   | 0.98307    |          |\n",
    "| fasttext embed+2xBiLstm (minimal process) | 0.98459   | 0.98443    |          |\n",
    "| fasttext embed+2xBiLstm (regex preprocess)| 0.98581   | 0.98550    |          |\n",
    "\n",
    "- And The Best score was mean roc_auc of 0.98619 (Public LB) & 0.98602 (Private LB) from the ensemble of predictions from 7 different models & best single model is based on the Bidirectional LSTM with fasttext pretrained embeddings.\n",
    "\n",
    "### what top scorers have done differently:\n",
    "\n",
    "The below mentioned techniques althogh not very significant, known to increased scores on test-set around 0.01-0.02.(high score: mean roc-auc of 0.9889)\n",
    "\n",
    "- Bert: This is the state of art of model for various text classification tasks. This model is mainly based on the \"Transformers.\" & can be used for transfer learning.\n",
    "- Training for more epoch with K-Fold cross validation: Although validation score is constantly increasing...training more for 50 to 100 epcohs with early stopping gave better results to some extent (around 0.01 variation in test scores).\n",
    "**Although the above two steps look simple to follow, Due to hardware & kaggle kernel runtime limitaions I didn't get chance to try these.**\n",
    "\n",
    "- Back translation: A way to augument text data. They translated the train data into different using translation services & then back translated them again into English. This produces slightly different texts than original.\n",
    "- Test time augumentation: Same Back translation method is followed on the test data and final scores were average of predictions on different dataset variations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
